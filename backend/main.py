from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from sentence_transformers import SentenceTransformer
import ollama
import faiss
import numpy as np
import pandas as pd
import requests  # To call Ollama

app = FastAPI()

# Configure CORS to avoid frontend errors
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load data and embeddings model
df = pd.read_csv("data/products.csv")
# model = SentenceTransformer('all-MiniLM-L6-v2')
model = SentenceTransformer('nomic-ai/nomic-embed-text-v1', trust_remote_code=True)  # all-MiniLM model
index = faiss.read_index("embeddings/product_embeddings.faiss")

def query_ollama(prompt: str) -> str:
    try:
        response = ollama.chat(
            model='mistral',
            messages=[
                {
                    'role': 'user',
                    'content': prompt,
                },
            ]
        )
        
        # Extract the content of the response
        response_content = response['message']['content']
        
        return response_content
    
    except Exception as e:
        # Handle any errors that may occur
        print(f"❌ Error: {e}")  # <-- DEBUG
        return f"❌ Error: {e}"

@app.post("/recommend")
async def recommend(request: Request):
    data = await request.json()
    query = data.get("query")

    # 1️⃣ Generate embedding for the query
    query_embedding = model.encode([query])
    query_embedding = np.array(query_embedding).astype('float32')

    # Normalize the query embedding
    query_embedding = query_embedding / np.linalg.norm(query_embedding)

    # 2️⃣ Search for similar products in the database
    k = 3  # Number of results
    similarities, indices = index.search(query_embedding, k)  # Using Inner Product (cosine similarity)

    # 3️⃣ Prepare list of recommended products
    results = []
    for i, idx in enumerate(indices[0]):
        product = df.iloc[idx]
        similarity = float(similarities[0][i])  # Cosine similarity (already normalized)
        results.append({
            "ID": int(product["ID"]),
            "Person Name": str(product["Person Name"]),
            "Description": str(product["Description"]),
            "Similarity": similarity
        })

    # 4️⃣ Call Ollama to generate response
    ollama_prompt = (
        f"Dado el siguiente criterio de búsqueda: '{query}', se han encontrado varios candidatos por un modelo de LLM pre-entrenado llamado nomic-ai/nomic-embed-text-v1. tu función es dar una breve introducción sobre los candidatos que se encontraron basado en el prompt que recibes en el query, como tal no tienes información sobre los candidatos, por lo que debes ser creativo y generar texto coherente. Recuerda que el modelo de lenguaje es muy poderoso y puede generar texto de alta calidad. Por favor, genera un texto de 3-5 oraciones sobre los candidatos encontrados, no digas nombres ni detalles específicos, ya que no sabes el nombre de los candidatos solo cosas como ¡Aquí hay una variedad de talentosos... y cosas de similar índole"
    )
    llm_text = query_ollama(ollama_prompt)

    # 5️⃣ Return response with products and the text generated by Ollama
    return {
        "query": query,
        "llm_response": llm_text,
        "results": results
    }
